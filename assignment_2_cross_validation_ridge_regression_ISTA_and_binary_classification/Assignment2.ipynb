{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYU Introduction to Machine Learning\n",
    "## Assignment 2\n",
    "\n",
    "__<font color='red'>Given date: February 20</font>__\n",
    "\n",
    "__<font color='red'>Due Date: March 8</font>__\n",
    "\n",
    "__Total: 25pts__\n",
    "\n",
    "In this assignment you will implement the main regularization approaches as well as cross validation. You will study how the OLS criterion that we used in regression can be extended to classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1. Understanding Cross validation (5pts)\n",
    "\n",
    "Using the lines below load the dataset 'Assignment2_Ex1_xi' and 'Assignment2_Ex1_ti'. Each of the points in the training set is represented by 5 features $x_{i,1}$, $x_{i,2}, \\ldots x_{i,5}$. Among those features we want to find those which are the most meaningful to the description of the targets $t_i$. You can think of the targets as expressing for example the probability to develop a particular trait or disease and the features as encoding the expressivity of particular genes. In such a framework the objective would thus mean finding the genes that most influence the particular trait. For this we will implement a Best Subset Selection approach with cross validation. Complete the cell below by implementing the following steps\n",
    "\n",
    "__1.__ For each number of weights (d=1 to 5) compute all the subsets (beta_i, beta_j, ...) of size d of weights.   \n",
    "__2.__ Split the training set in K=5 bins, for each bin k=1,...5, learn the weights by using the linear_regression function of scikit learn (do not reimplement gradient descent except if you really have too much time). Learn the weights on the remaining K-1 bins then comute the MSE on bin k. \n",
    "__3.__ Find the optimal subset of coefficients by comparing the MSE and plot the MSE as a function of the number k of weights by averaging the errors over the size k subsets. I.e MSE(1) = (1/5)(MSE(beta0) + MSE(beta1) + ...MSE(beta4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Minimum MSEs: \n",
      "Subset of size 2 : [0.79553122]\n",
      "Subset of size 3 : [2.7323814e-07]\n",
      "Subset of size 4 : [5.87230504e-08]\n",
      "Subset of size 5 : [2.40519384e-08]\n",
      "Subset of size 6 : [4.06038444e-29]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import combinations \n",
    "\n",
    "xi = np.load('Assignment2_Ex1_xi.npy')\n",
    "ti = np.load('Assignment2_Ex1_ti.npy')\n",
    "\n",
    "# print(xi)\n",
    "# print(ti)\n",
    "\n",
    "D = 5 # number of coefficients \n",
    "\n",
    "K = 5 # number of bins used for cross validation\n",
    "\n",
    "# Note that K does not have to be equal to D \n",
    "# (this is a choice we make here but we could have taken any value for K)\n",
    "\n",
    "# Step 1: Finding the optimal d\n",
    "\n",
    "# initialization\n",
    "MSE = np.zeros((D,1))\n",
    "# print(MSE)\n",
    "\n",
    "optimal_betas = [0,0,0,0,0,0]\n",
    "optimal_MSE = -1\n",
    "\n",
    "# for each subset's size\n",
    "for d in np.arange(1,D+1): \n",
    "    # d = 1 ... 5 is the size of the subset of xi\n",
    "    # the corresponsing subset of betas is of size d + 1 \n",
    "    # print(\"size\", d)\n",
    "    \n",
    "    # compute all the subsets of size d\n",
    "    comb = combinations(np.arange(0,D), d)\n",
    "    comb_number = 0\n",
    "    \n",
    "    # initial large value for the minimal MSE of the subsets of size d+1\n",
    "    MSE_tmp = 10000 \n",
    "    for combination in list(comb): # for each compination of size d\n",
    "        comb_number +=1\n",
    "        # print (combination)\n",
    "        \n",
    "        # add a column of 1 (for beta0 constanst)\n",
    "        phi = np.ones((len(xi),1))\n",
    "        \n",
    "        # choose the xi corresponding to the combination\n",
    "        for elem in np.arange(0,d):\n",
    "            phi = np.hstack((phi, xi[:, [combination[elem]]]))   \n",
    "        # print(phi) # this is the data we will use for this combination of betas\n",
    "        # print(np.shape(phi))\n",
    "\n",
    "        # split into k bins\n",
    "        # assume that k divides len(xi)\n",
    "        lenght_bin = len(xi)/K\n",
    "\n",
    "        # MSE for the set of betas selected of size d\n",
    "        MSE_tmp2 = 0\n",
    "        \n",
    "        # for each red bin (testing bin for which we compute MSE)\n",
    "        for testing_bin_number in np.arange(0, K): \n",
    "            # set the test bin \n",
    "            phi_test = phi[int(testing_bin_number*lenght_bin) : int((testing_bin_number + 1)*lenght_bin), :]\n",
    "            ti_test = ti[int(testing_bin_number*lenght_bin) : int((testing_bin_number + 1)*lenght_bin), :]\n",
    "            # print(\"test\")\n",
    "            # print(phi_test)\n",
    "            # print(ti_test)\n",
    "\n",
    "            # set the training bins\n",
    "            phi_training = np.delete(phi, slice(int(testing_bin_number*lenght_bin), int((testing_bin_number + 1)*lenght_bin)), 0)\n",
    "            ti_training = np.delete(ti, slice(int(testing_bin_number*lenght_bin), int((testing_bin_number + 1)*lenght_bin)), 0)\n",
    "            # print(\"training\")\n",
    "            # print(phi_training)\n",
    "            # print(ti_training)\n",
    "\n",
    "            # learn the model on the training bins\n",
    "            regression = LinearRegression()\n",
    "            regression.fit(phi_training, ti_training)\n",
    "            \n",
    "            # use the model to predict the test bin's targets\n",
    "            prediction = regression.predict(phi_test)\n",
    "            # print(\"true targets\")\n",
    "            # print(ti_test)\n",
    "            # print(\"prediction\")\n",
    "            # print(prediction)\n",
    "            \n",
    "            # calculate the difference between the predicted targets and the real targets\n",
    "            error = ti_test - prediction\n",
    "            # print(\"error\")\n",
    "            # print(error)\n",
    "            # print(np.power(error, 2))\n",
    "            \n",
    "            MSE_tmp2 += np.sum(np.power(error, 2))\n",
    "            # print(MSE_tmp2)\n",
    "            \n",
    "        # MSE for this combination of beta done\n",
    "        # add it to the MSE for all subsets of the same size d (divide by N)\n",
    "        MSE_tmp = min(np.true_divide(MSE_tmp2, len(xi)), MSE_tmp) \n",
    "\n",
    "\n",
    "    # print(comb_number)\n",
    "    # MSE[d-1] = np.true_divide(MSE_tmp, comb_number)\n",
    "    MSE[d-1] = MSE_tmp\n",
    "            \n",
    "\n",
    "# Step 2 plotting the evolution of the average prediction error as a function of the number of coefficient\n",
    "print(\"The Minimum MSEs: \")\n",
    "for i in range(len(MSE)):\n",
    "    print(\"Subset of size\", i+2, \": \", end = \"\")\n",
    "    print(MSE[i]) \n",
    "     \n",
    "graph = plt.figure()\n",
    "fig = graph.add_subplot()\n",
    "fig.set_xlabel(\"Subset size\")\n",
    "fig.set_ylabel('MSE')\n",
    "plt.scatter(np.arange(2, D+2), MSE) # set of 2 to 6 betas (meaning 1 to 5 features)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2. Predicting graduate admissions (5pts)\n",
    "\n",
    "In this second question, we want to predict admission to graduate school based on a collection of features [provided by Kaggle](https://www.kaggle.com/mohansacharya/graduate-admissions) \n",
    "including: \n",
    "\n",
    "- GRE and TOEFL Scores\n",
    "- University Rating \n",
    "- Letter of Recommendation Strength \n",
    "- Undergraduate GPA \n",
    "- ...\n",
    "    \n",
    "We want to learn a ridge regression model (use the scikit learn model with the fit and predict functions). \n",
    "\n",
    "- Start by splitting the dataset into a training (about 90%) an a test (remaining 10%) parts using a call to the train_test_split function from the model_selection module. Put the test aside for the rest of the exercise. \n",
    "\n",
    "- Now that you are perfectly comfortable with the idea of cross validation, we will also try to evaluate the optimal lambda in the Ridge regression model. For this, you can use an extension of scikit learn Ridge regression model: sklearn.linear_model.RidgeCV. This extension lets you specify an array of $\\lambda$ values ($\\alpha$ in scikit learn) to try. The best value is then returned through a call to the 'alpha_' attribute of the model (read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) for more details). Train the model (both lambda and beta) on the training subset of item 1.\n",
    "\n",
    "- Finally evaluate the prediction of your model on the 10% test set you kept on the side at the beginning. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:\n",
      "0.7981818181818182\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"Admission_Predict.csv\")\n",
    "# print(df)\n",
    "\n",
    "# split targets and features\n",
    "xi = df.drop(['Chance of Admit '],axis=1) # independant features\n",
    "ti = df['Chance of Admit '] # targets\n",
    "\n",
    "# split into training and test set\n",
    "xi_train, xi_test, ti_train, ti_test = train_test_split(xi, ti, test_size=0.10, random_state=42)\n",
    "\n",
    "# build Xtilde\n",
    "Xtilde_train = np.hstack((np.ones((len(xi_train), 1)), xi_train))\n",
    "\n",
    "# learn the regression model\n",
    "regression = RidgeCV(alphas=np.linspace(0.001,1, 100).tolist()).fit(Xtilde_train, ti_train)\n",
    "\n",
    "# predict for the test set\n",
    "Xtilde_test = np.hstack((np.ones((len(xi_test), 1)), xi_test))\n",
    "prediction = regression.predict(Xtilde_test)\n",
    "\n",
    "# plot the result\n",
    "targets = np.array(ti_test)\n",
    "graph = plt.figure()\n",
    "fig = graph.add_subplot()\n",
    "fig.set_xlabel(\"Student\")\n",
    "fig.set_ylabel('Probability graduate admission')\n",
    "plt.bar(np.arange(1, len(prediction)+1), targets, width = 0.6, align = \"edge\")\n",
    "plt.bar(np.arange(1, len(prediction)+1), prediction, width = 0.5, color= 'r')\n",
    "plt.legend([\"Real Target \", \"Predicted Target\"])\n",
    "plt.show()\n",
    "\n",
    "# print(regression.score(xi_train_spe.values.reshape(-1,1), ti_train))\n",
    "print(\"Alpha:\")\n",
    "print(regression.alpha_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Iterative Hard and Soft Thresholding (10pts)\n",
    "\n",
    "\n",
    "An alternative to the simple OLS criterion or to the Ridge regression model, LASSO regression minimizes a combination of a data fidelity term and a penalty on the sum of the absolute values of the regression coefficients, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\boldsymbol \\beta ) = \\frac{1}{N}\\sum_{i=1}^N (t^{(i)} - (\\boldsymbol{\\beta}^T \\boldsymbol x^{(i)}))^2 + \\lambda \\sum_{j=0}^D |\\beta_j|, \\quad (\\text{LASSO})\n",
    "\\end{align}\n",
    "\n",
    "One of the main difficulty with the LASSO lies in the non differentiability of the absolute value which appears in the regularization term. Because of the use of the absolute value, the gradient cannot be computed at 0. Instead of relying on gradient updates, we can instead turn to the constrained formulation\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\min & \\quad \\ell(\\boldsymbol \\beta ) = \\frac{1}{N}\\sum_{i=1}^N (t^{(i)} - (\\boldsymbol{\\beta}^T \\boldsymbol x^{(i)}))^2\\\\\n",
    "\\text{subject to}& \\quad \\sum_{j=0}^D |\\beta_j|\\leq t\n",
    "\\end{align}\n",
    "\n",
    "The drawback with such a formulation is that we now have to solve a constrained problem. A common approach relies on the use of thresholding algorithms and in particular to the class of so-called _iterative shrinkage-thresholding algorithms (ISTA)_. If we write the OLS objective in matrix form as $\\ell(\\boldsymbol \\beta) = \\frac{1}{2N}\\|\\tilde{\\mathbf{X}}\\mathbf{\\beta} - \\mathbf{t}\\|_2^2$, Iterative shrinkage-thresholding algorithms are based on the following update :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\beta} \\leftarrow \\mathcal{T}_{\\lambda \\eta}\\left\\{\\mathbf{\\beta} - 2\\eta \\tilde{\\mathbf{X}}^T\\left(\\tilde{\\mathbf{X}}\\mathbf{\\beta} - \\mathbf{t}\\right) \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{T}$ is the thresholding operator \n",
    "\\begin{align}\n",
    "\\mathcal{T}_{\\alpha}(\\mathbf{\\beta})_i = \\left(|\\beta_i| - \\alpha\\right)_+\\text{sign}(\\beta_i)\n",
    "\\end{align}\n",
    "\n",
    "Here $\\left(|\\beta_i| - \\alpha\\right)_+ = \\max\\left\\{|\\beta_i| - \\alpha, 0\\right\\}$ and $\\text{sign}(\\beta_i)$ denotes the sign of the coefficient $\\beta_i$. From the definition above, you can also see that $\\lambda \\eta$ acts as a threshold on the $\\beta_i$. The larger $\\lambda$, the more $\\beta_i$ will be set to $0$. \n",
    "\n",
    "\n",
    "#### Question 3.1 (6pts) Complete the function ISTA below which should return a vector of weights $\\mathbf{\\beta}$, starting from some initial guess $\\beta_{\\text{init}}$ and for a training set stored in the matrix $X$ and vector of targets $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# function should apply the Iterative Shrinkage \n",
    "# Thresholding updates, starting from Beta_init and \n",
    "# for a set of feature vectors stored in matrix X \n",
    "# with associated targets stored in t.\n",
    "\n",
    "def ISTA(beta_init, lamb, eta, X, t):\n",
    "    # create X_tilde\n",
    "    Xtilde = np.hstack((np.ones((len(X), 1)), X))\n",
    "    \n",
    "    # maximum number of iterations for the main for loop\n",
    "    max_iter = 100\n",
    "    \n",
    "    # beta init\n",
    "    beta_ISTA = beta_init\n",
    "    \n",
    "    # main loop\n",
    "    for iter in np.arange(0, max_iter):\n",
    "\n",
    "        XB = np.matmul(Xtilde, beta_ISTA.reshape(-1,1))\n",
    "        # print(XB)\n",
    "\n",
    "        XB_t = XB - t.reshape(-1,1)\n",
    "        # print(XB_t)\n",
    "        \n",
    "        TwoEtaXtilde = 2 * eta * Xtilde.T\n",
    "        # print(TwoEtaXtilde)\n",
    "        \n",
    "        product = np.matmul(TwoEtaXtilde, XB_t)\n",
    "        # print(product)\n",
    "\n",
    "        tmp = np.true_divide(beta_ISTA.reshape(-1,1) - product, len(X))\n",
    "        # print(tmp)\n",
    "\n",
    "        beta_ISTA = np.maximum(np.absolute(tmp)-lamb, np.zeros(len(tmp)).reshape(-1,1)) * np.sign(tmp) \n",
    "        # print(beta_ISTA)\n",
    "        \n",
    "    return beta_ISTA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.2 (4pts) Test your algorithm on the dataset provided by the two files 'Assignment2_Ex32_Xi.npy' and 'Assignment2_Ex32_ti.npy' below. as above, two of the weights are irrelevant. Try to tune the parameters $\\eta$ and $\\lambda$ and study when you can recover those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_ISTA:\n",
      "[[-0.        ]\n",
      " [ 0.45779933]\n",
      " [ 0.12151702]\n",
      " [-0.        ]\n",
      " [ 0.15831574]\n",
      " [-0.00068004]]\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the data\n",
    "X = np.load('Assignment2_Ex32_Xi.npy')\n",
    "t = np.load('Assignment2_Ex32_ti.npy')\n",
    "\n",
    "# start with a random beta\n",
    "beta_init = np.random.normal(0,1,len(X[0])+1) \n",
    "\n",
    "# weight for lasso regularization\n",
    "lamb = 0.05 \n",
    "\n",
    "# learning rate\n",
    "eta = 0.2 \n",
    "\n",
    "# compute the model\n",
    "beta_ISTA = ISTA(beta_init, lamb, eta, X, t)\n",
    "\n",
    "# print the model\n",
    "print(\"Beta_ISTA:\")\n",
    "print(beta_ISTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (5pts). From regression to binary classification: Predicting deaths on the Titanic\n",
    "\n",
    "We have seen how the OLS objective can be used to learn a regression model. This objective remains in fact absolutely valid in the classification framework. In binary classification, the targets associated to the feature vectors take one of two values (let us say $1$ and $0$ or $+1$ and $-1$). If we want to learn a model that classifies some feature vectors $\\mathbf{x}^{(i)}$ as belonging to class $\\mathcal{C}_0$ vs $\\mathcal{C}_1$ and we are given a training set $C_{0, \\text{tr}} = \\left\\{\\mathbf{x}^{(i)}\\right\\}_{i=1}^{N_0}$ and $C_{1, \\text{tr}} = \\left\\{\\mathbf{x}^{(j)}\\right\\}_{j=1}^{N_1}$, we can try to learn a separating plane $\\beta_0 +\\beta_1 x_1 + \\ldots \\beta_D x_D$ such that $\\beta_0 +\\beta_1 x^{(i)}_1 + \\ldots \\beta_D x^{(i)}_D =+1 $ for all $x^{(i)}\\in C_0$ and $\\beta_0 +\\beta_1 x^{(j)}_1 + \\ldots \\beta_D x^{(j)}_D =-1$ for all $x^{(j)}$ in $\\mathcal{C}_1$. \n",
    "\n",
    "For any new point $\\mathbf{x}$ of unknown class, we can then compute $\\beta_0 +\\beta_1x_1 + \\ldots +\\beta_D x_D$ and classify our point as belonging to $C_0$ if $\\beta_0 +\\beta_1x_1 + \\ldots +\\beta_D x_D>0$.\n",
    "\n",
    "Combine this idea with the linear regression model from scikit learn to learn a linear binary classifier for the ['Titanic'](https://www.kaggle.com/c/titanic/data?select=test.csv) dataset from Kaggle. Start by loading the training and test data from this dataset and then complete the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction's accuracy is 81.11111111111111 %\n",
      "\n",
      "Moubarek, Master. Halim Gonios (\"William George\"): \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Kvillner, Mr. Johan Henrik Johannesson: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Alhomaki, Mr. Ilmari Rudolf: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Harper, Miss. Annie Jessie \"Nina\": \u001b[1m survived \u001b[0m (1)\n",
      "Nicola-Yarred, Miss. Jamila: \u001b[1m survived \u001b[0m (1)\n",
      "Barber, Miss. Ellen \"Nellie\": \u001b[1m survived \u001b[0m (1)\n",
      "Kelly, Miss. Anna Katherine \"Annie Kate\": \u001b[1m survived \u001b[0m (1)\n",
      "Vander Planke, Mr. Leo Edmondus: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Carr, Miss. Helen \"Ellen\": \u001b[1m survived \u001b[0m (1)\n",
      "Newsom, Miss. Helen Monypeny: \u001b[1m survived \u001b[0m (1)\n",
      "Futrelle, Mr. Jacques Heath: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Kelly, Mr. James: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Lefebre, Miss. Jeannie: \u001b[1m survived \u001b[0m (1)\n",
      "Attalah, Mr. Sleiman: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Fox, Mr. Stanley Hubert: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Lines, Miss. Mary Conover: \u001b[1m survived \u001b[0m (1)\n",
      "Kimball, Mr. Edwin Nelson Jr: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "O'Leary, Miss. Hanora \"Norah\": \u001b[1m survived \u001b[0m (1)\n",
      "Montvila, Rev. Juozas: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Porter, Mr. Walter Chamberlain: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Mineff, Mr. Ivan: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Seward, Mr. Frederic Kimber: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Andersen-Jensen, Miss. Carla Christine Nielsine: \u001b[1m survived \u001b[0m (1)\n",
      "Olsvigen, Mr. Thor Anderson: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Risien, Mr. Samuel Beard: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Skoog, Master. Karl Thorsten: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Uruchurtu, Don. Manuel E: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Wilhelms, Mr. Charles: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Skoog, Master. Harald: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Olsson, Miss. Elina: \u001b[1m survived \u001b[0m (1)\n",
      "Petroff, Mr. Nedelio: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Nysten, Miss. Anna Sofia: \u001b[1m survived \u001b[0m (1)\n",
      "Hoyt, Mr. William Fisher: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Harknett, Miss. Alice Phoebe: \u001b[1m survived \u001b[0m (1)\n",
      "Cohen, Mr. Gurshon \"Gus\": \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Yasbeck, Mr. Antoni: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Sloper, Mr. William Thompson: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Madigan, Miss. Margaret \"Maggie\": \u001b[1m survived \u001b[0m (1)\n",
      "Serepeca, Miss. Augusta: \u001b[1m survived \u001b[0m (1)\n",
      "Nankoff, Mr. Minko: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Hickman, Mr. Stanley George: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Braund, Mr. Lewis Richard: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Sirota, Mr. Maurice: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Keane, Mr. Andrew \"Andy\": \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Barbara, Mrs. (Catherine David): \u001b[1m survived \u001b[0m (1)\n",
      "Ford, Mr. William Neal: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Crease, Mr. Ernest James: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Odahl, Mr. Nils Martin: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Ali, Mr. Ahmed: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Newell, Mr. Arthur Webster: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Baclini, Miss. Marie Catherine: \u001b[1m survived \u001b[0m (1)\n",
      "Hays, Mrs. Charles Melville (Clara Jennings Gregg): \u001b[1m survived \u001b[0m (1)\n",
      "Skoog, Mr. Wilhelm: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Carter, Master. William Thornton II: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Duane, Mr. Frank: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Taussig, Miss. Ruth: \u001b[1m survived \u001b[0m (1)\n",
      "Jenkin, Mr. Stephen Curnow: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Baxter, Mrs. James (Helene DeLaudeniere Chaput): \u001b[1m survived \u001b[0m (1)\n",
      "Cameron, Miss. Clear Annie: \u001b[1m survived \u001b[0m (1)\n",
      "Devaney, Miss. Margaret Delia: \u001b[1m survived \u001b[0m (1)\n",
      "Birkeland, Mr. Hans Martin Monsen: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Mellinger, Miss. Madeleine Violet: \u001b[1m survived \u001b[0m (1)\n",
      "Clarke, Mrs. Charles V (Ada Maria Winfield): \u001b[1m survived \u001b[0m (1)\n",
      "Baumann, Mr. John D: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Mernagh, Mr. Robert: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Mack, Mrs. (Mary): \u001b[1m survived \u001b[0m (1)\n",
      "Hood, Mr. Ambrose Jr: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Zimmerman, Mr. Leo: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Elias, Mr. Joseph Jr: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Stephenson, Mrs. Walter Bertram (Martha Eustis): \u001b[1m survived \u001b[0m (1)\n",
      "Lahtinen, Mrs. William (Anna Sylfven): \u001b[1m survived \u001b[0m (1)\n",
      "Ryerson, Miss. Emily Borie: \u001b[1m survived \u001b[0m (1)\n",
      "Behr, Mr. Karl Howell: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Compton, Miss. Sara Rebecca: \u001b[1m survived \u001b[0m (1)\n",
      "Augustsson, Mr. Albert: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Lindell, Mr. Edvard Bengtsson: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Mangan, Miss. Mary: \u001b[1m survived \u001b[0m (1)\n",
      "Frolicher, Miss. Hedwig Margaritha: \u001b[1m survived \u001b[0m (1)\n",
      "Funk, Miss. Annie Clemmer: \u001b[1m survived \u001b[0m (1)\n",
      "Rosblom, Mrs. Viktor (Helena Wilhelmina): \u001b[1m survived \u001b[0m (1)\n",
      "Panula, Mr. Ernesti Arvid: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Nye, Mrs. (Elizabeth Ramell): \u001b[1m survived \u001b[0m (1)\n",
      "Spedden, Mrs. Frederic Oakley (Margaretta Corning Stone): \u001b[1m survived \u001b[0m (1)\n",
      "Kilgannon, Mr. Thomas J: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Smith, Mr. James Clinch: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Artagaveytia, Mr. Ramon: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Newell, Miss. Madeleine: \u001b[1m survived \u001b[0m (1)\n",
      "Francatelli, Miss. Laura Mabel: \u001b[1m survived \u001b[0m (1)\n",
      "Reuchlin, Jonkheer. John George: \u001b[1m did NOT survive \u001b[0m (0)\n",
      "Reed, Mr. James George: \u001b[1m did NOT survive \u001b[0m (0)\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# split targets and features\n",
    "xi = data.drop(['Survived'],axis=1) # features\n",
    "ti = data['Survived'] # targets\n",
    "\n",
    "# split into training set (90%) and test set (10%)\n",
    "xi_train_tmp, xi_test_tmp, ti_train_tmp, ti_test_tmp = train_test_split(xi, ti, test_size=0.10, random_state=42)\n",
    "\n",
    "# turn possible non numeric features to numbers and remove lines with nAn\n",
    "def clean_xi(dataset):\n",
    "    dataSet = dataset.copy(deep = True)\n",
    "    for index, row in dataset.iterrows():\n",
    "        if row['Sex'] == 'female':\n",
    "            dataSet.loc[index, 'Sex'] = 0\n",
    "        else:\n",
    "            dataSet.loc[index, 'Sex'] = 1\n",
    "    \n",
    "    # remove useless columns\n",
    "    # I choose to remove the column Age as there are too many nan values (162). \n",
    "    # removing 162 lines is removing too much information\n",
    "    # As you told me ,I also decided to remove the columns that did not influence the model\n",
    "    dataSet = dataSet.drop(['Name', \"Ticket\",\"Age\",\"Cabin\", \"Embarked\"],axis=1)\n",
    "    return dataSet\n",
    "\n",
    "# turn the class targets to binary or +1/-1 values.\n",
    "def clean_ti(dataset):\n",
    "    for index, row in dataset.iteritems():\n",
    "        if row == 0:\n",
    "            dataset.loc[index] = -1;\n",
    "    return dataset\n",
    "\n",
    "# # Step 1. \n",
    "# # =========================================================================\n",
    "# # Use the linearRegression model from scikit learn with binary \n",
    "# # targets to predict the passengers that will survive and die in the \n",
    "# # case of the sinking of a ship. Start by turning the class targets to \n",
    "# # binary or +1/-1 values. Then turn possible non numeric features to numbers. Finally \n",
    "# # learn the separating plane.\n",
    "\n",
    "ti_train = clean_ti(ti_train_tmp)\n",
    "xi_train = clean_xi(xi_train_tmp)\n",
    "   \n",
    "\n",
    "# learn with the linear regression model from scikit learn\n",
    "regression = LinearRegression()\n",
    "regression.fit(xi_train.to_numpy(), ti_train.values.reshape(-1,1))\n",
    "\n",
    "# Step 2. \n",
    "# =========================================================================\n",
    "# Validate your model on the test set and compute the fraction of correctly \n",
    "# classified samples using the function accuracy_score from the sklearn.metrics module\n",
    "\n",
    "# import\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xi_test = clean_xi(xi_test_tmp) # part of the data used to predict the result\n",
    "\n",
    "# predict the targets\n",
    "prediction = regression.predict(xi_test.to_numpy())\n",
    "\n",
    "# associate a binary number with the prediction\n",
    "for predicted in prediction:\n",
    "    if predicted[0]<0:\n",
    "        predicted[0] = 0\n",
    "    else:\n",
    "        predicted[0] = 1\n",
    "\n",
    "\n",
    "score = accuracy_score(ti_test_tmp, prediction) * 100\n",
    "print(\"The prediction's accuracy is \" + str(score) + \" %\\n\")   \n",
    "\n",
    "# display data\n",
    "ind = 0\n",
    "for index, row in xi_test_tmp.iterrows():  \n",
    "    print(row['Name'], end = ': ')\n",
    "    if prediction[ind][0] == 1:\n",
    "        print(\"\\033[1m survived \\033[0m (1)\")\n",
    "    else:\n",
    "        print(\"\\033[1m did NOT survive \\033[0m (0)\")\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
